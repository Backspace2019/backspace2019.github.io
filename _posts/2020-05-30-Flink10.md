# Flink课程学习笔记

## 1.Flink流处理简介

### Flink和Spark的区别

## 2.第一个Flink程序

### IDEA中使用Flink-wordcount



### 配置IDEA

 ```
1. 使用Intellij IDEA创建一个Maven新项目
2. 勾选Create from archetype，然后点击Add Archetype按钮
3. GroupId中输入org.apache.flink，ArtifactId中输入flink-quickstart-scala，Version中输入1.10.0，然后点击OK
4. 点击向右箭头，出现下拉列表，选中flink-quickstart-scala:1.10.0，点击Next
5. Name中输入FlinkTutorial，GroupId中输入com.atguigu，ArtifactId中输入FlinkTutorial，点击Next
6. 最好使用IDEA默认的Maven工具：Bundled（Maven 3），点击Finish，等待一会儿，项目就创建好了
 ```

###   WordCount


```scala
package com.atguigu

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time // 导入所有的隐式类型转换

object WordCount {

  case class WordWithCount(word: String,
                           count: Int)

  def main(args: Array[String]): Unit = {
    // 获取运行时环境
    // 相当于sparkContext
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 设置并行度为1
    env.setParallelism(1)

    // 定义DAG
    // 数据流的来源（socket）=> 处理逻辑 => 数据流计算结果的去向，sink（print，打印到屏幕）
    val text = env

//      .fromElements(
//        "hello world",
//        "hello atguigu"
//      )
      .socketTextStream("localhost", 9999, '\n')

    val windowCount = text
      // 使用空格对字符串进行切分
      .flatMap(w => w.split("\\s"))
      // map操作
      .map(w => WordWithCount(w, 1))
      // 分组，shuffle操作
      .keyBy("word")
      // 开滚动窗口
      .timeWindow(Time.seconds(5))
      // reduce操作
      .sum("count")
    // 定义DAG结束
    
    // output操作，输出到标准输出，stdout
    windowCount
      .print()
    
    // 执行DAG
    env.execute()

  }
}
```

## 3.Flink程序打包并提交到运行时环境

### flink 下载

下载地址：http://mirror.bit.edu.cn/apache/flink/flink-1.10.1/flink-1.10.1-bin-scala_2.11.tgz

### flink安装

1. 把下载好的安装包放在/opt/software目录下
2. 把安装包解压到/opt/module目录下`tar -zxvf flink-1.10.1-bin-scala_2.11.tgz -C /opt/module/`
3. `cd /opt/module/flink-1.10.1`到安装目录执行`./bin/start-cluster.sh`
4. 在Windows端浏览器查看http://hadoop102:8081

### flink使用

1. 在IDEA中使用`Maven Package`打包

2. 提交打包好的`JAR`包,**也可以在网页端提交jar包**

   ```shell
   cd flink-1.10.1
   ./bin/flink run jar包的绝对路径
   ```

3. 停止flink集群

   ```shell
   cd flink-1.10.1
   ./bin/stop-cluster.sh
   ```

4. 查看标准日志输出的位置，在`log`文件夹中

   ```shell
   cd flink-1.10.1/log
   ```



## 4.flink运行时架构

#### 运行时组件

- 分布式系统遵循 `Master-Slave` `master - worker` `leader - follower` `Driver - Executor`主从架构，心跳通信
- 复习进程和线程
- 同步和异步

JobManager	作业管理器 [master] [Driver]

TaskManager	任务管理器 [slave] [Executor]

ResourceManager	资源管理器

Dispacher	分发器

#### 任务提交流程



#### 任务调度原理

## 5.Flink自定义数据源

## 6.Flink基本转换算子

## 7.Flink键控流转换算子

## 8.keyby滚动聚合

## 9.Flink多流转换算子

## 10.Flink-分布式转换算子

## 11.Flink的类型

## 12.keyby语法

## 13.Flink的UDF函数

## 14.Flink窗口操作符

## 15.Flink窗口增量聚合

## 16.Flink全窗口聚合

## 17.Flink增量聚合和全窗口聚合结合使用

## 18.Flink时间语义

## 19.Flink水位线

## 20.测试水位线插入时间间隔对窗口闭合和迟到数据的影响

## 21.周期性插入水位线底层实现讲解

## 22.两条流合并时水位线的产生逻辑（取小的水位线）

## 23.KeyedProcessFunction和定时器onTimer

## 24.连续一秒钟温度上升报警器

## 25.温度上升的状态变量讲解

## 26.侧输出的讲解

## 27.CoProcessFunction编程

## 28.触发器编程

## 29.将迟到数据发送到侧输出

## 附：课程代码

### Day01上午

```scala
package com.atguigu

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time // 导入所有的隐式类型转换

object WordCount {

  case class WordWithCount(word: String,
                           count: Int)

  def main(args: Array[String]): Unit = {
    // 获取运行时环境
    // 相当于sparkContext
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    // 设置并行度为1
    env.setParallelism(1)

    // 定义DAG
    // 数据流的来源（socket）=> 处理逻辑 => 数据流计算结果的去向，sink（print，打印到屏幕）
    val text = env

//      .fromElements(
//        "hello world",
//        "hello atguigu"
//      )
      .socketTextStream("localhost", 9999, '\n')

    val windowCount = text
      // 使用空格对字符串进行切分
      .flatMap(w => w.split("\\s"))
      // map操作
      .map(w => WordWithCount(w, 1))
      // 分组，shuffle操作
      .keyBy("word")
      // 开滚动窗口
      .timeWindow(Time.seconds(5))
      // reduce操作
      .sum("count")
    // 定义DAG结束
    
    // output操作，输出到标准输出，stdout
    windowCount
      .print()
    
    // 执行DAG
    env.execute()

  }
}
```

### Day01下午

```scala
package com.atguigu

import org.apache.flink.streaming.api.scala._

object StreamingJob {
  def main(args: Array[String]) {
    // set up the streaming execution environment
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val stream = env.fromElements(
      "hello world",
      "hello atguigu"
    )
    
    stream.print() //  stdout
    
    env.execute("Flink Streaming Scala API Skeleton")

  }
}
```

### Day02上午

```scala
package com.atguigu.datastreamapi

case class SensorReading(id: String,
                         timestamp: Long,
                         temperature: Double)
```



```scala
package com.atguigu.datastreamapi

import java.util.Calendar

import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction
import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext

import scala.util.Random

class SensorSource extends RichParallelSourceFunction[SensorReading] {

  var running = true

  override def run(sourceContext: SourceContext[SensorReading]): Unit = {
    val rand = new Random()

    var curFTemp = (1 to 10).map(
      i => ("sensor_" + i, 65 + (rand.nextGaussian() * 20))
    )
    
    while (running) {
    
      curFTemp = curFTemp.map(
        t => (t._1, t._2 + (rand.nextGaussian() * 0.5))
      )
    
      val curTime = Calendar.getInstance.getTimeInMillis
    
      curFTemp.foreach(t => sourceContext.collect(
        SensorReading(t._1, curTime, t._2)
      ))
    
      Thread.sleep(100)
    }

  }

  override def cancel(): Unit = running = false
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._

object SourceExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    env.setParallelism(1)

//    val stream = env
//      .fromElements(
//        SensorReading("sensor_1", 1547718199, 35.80018327300259),
//        SensorReading("sensor_6", 1547718199, 15.402984393403084),
//        SensorReading("sensor_7", 1547718199, 6.720945201171228),
//        SensorReading("sensor_10", 1547718199, 38.101067604893444)
//      )

    val stream = env
        .addSource(
          new SensorSource
        )
    
    stream.print()
    
    env.execute()

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.api.common.functions.MapFunction
import org.apache.flink.streaming.api.scala._

object MapExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val stream = env
      .addSource(new SensorSource)
      .map(new MyMapFunction)
//        .map(
//          new MapFunction[SensorReading, String] {
//            override def map(value: SensorReading): String = value.id
//          }
//        )

    stream.print()
    
    env.execute()

  }

  class MyMapFunction extends MapFunction[SensorReading, String] {
    override def map(value: SensorReading): String = value.id
  }
}
```



```
package com.atguigu.datastreamapi

import org.apache.flink.api.common.functions.FilterFunction
import org.apache.flink.streaming.api.scala._

object FilterExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val stream = env
      .addSource(new SensorSource)
//      .filter(r => r.id == "sensor_1")
        .filter(new MyFilterFunction)

    stream.print()
    
    env.execute()

  }

  class MyFilterFunction extends FilterFunction[SensorReading] {
    override def filter(value: SensorReading): Boolean = value.id == "sensor_1"
  }

}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.api.common.functions.FlatMapFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

object FlatMapExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val stream = env
//      .addSource(new SensorSource)
        .fromElements(
          1L, 2L, 3L
        )
        .flatMap(new MyFlatMapFunction)

    stream.print()
    
    env.execute()

  }

  class MyFlatMapFunction extends FlatMapFunction[Long, Long] {
    override def flatMap(value: Long, out: Collector[Long]): Unit = {
      if (value == 2) {
        out.collect(value)
      } else if (value == 3) {
        out.collect(value)
        out.collect(value)
      }
    }
  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._

object KeybyExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val stream : DataStream[SensorReading] = env
      .addSource(new SensorSource)

    val keyed : KeyedStream[SensorReading, String] = stream.keyBy(_.id)
    
    val min : DataStream[SensorReading] = keyed.min("temperature")
    
    min.print()
    
    env.execute()

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._

object RollingAggregateExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val inputStream: DataStream[(Int, Int, Int)] = env.fromElements(
      (1, 2, 2), (2, 3, 1), (2, 2, 4), (1, 5, 3))
    
    val resultStream: DataStream[(Int, Int, Int)] = inputStream
      .keyBy(0) // key on first field of the tuple
      .sum(1)   // sum the second field of the tuple in place
    
    resultStream.print()
    
    env.execute()

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._

object ReduceExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val stream = env
      .addSource(new SensorSource)
    

    val keyed = stream.keyBy(r => r.id)
    
    val min = keyed
      .reduce((x, y) => SensorReading(x.id, x.timestamp, x.temperature.min(y.temperature)))
    
    min.print()
    
    env.execute()

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._

object ReduceExample1 {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    val inputStream: DataStream[(String, List[String])] = env.fromElements(
      ("en", List("tea")), ("fr", List("vin")), ("en", List("cake")))

    val resultStream: DataStream[(String, List[String])] = inputStream
      .keyBy(0)
      .reduce((x, y) => (x._1, x._2 ::: y._2))
    
    resultStream.print()
    
    env.execute()

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._

object UnionExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val parisStream: DataStream[SensorReading] = env
      .addSource(new SensorSource)
      .filter(r => r.id == "sensor_1")
    val tokyoStream: DataStream[SensorReading] = env
      .addSource(new SensorSource)
      .filter(r => r.id == "sensor_2")
    val rioStream: DataStream[SensorReading] = env
      .addSource(new SensorSource)
      .filter(r => r.id == "sensor_3")
    val allCities: DataStream[SensorReading] = parisStream
      .union(tokyoStream, rioStream)
    
    allCities.print()
    
    env.execute()

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.functions.co.CoMapFunction
import org.apache.flink.streaming.api.scala._

object ConnectExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val one: DataStream[(Int, Long)] = env
      .fromElements(
        (1, 1L),
        (2, 2L),
        (3, 3L)
      )
    val two: DataStream[(Int, String)] = env
      .fromElements(
        (1, "1"),
        (2, "2"),
        (3, "3")
      )

    val connected : ConnectedStreams[(Int, Long), (Int, String)] = one
      .keyBy(_._1)
      .connect(
        two
          .keyBy(_._1)
      )
    
    val comap = connected
      .map(new MyCoMap)
    
    comap.print()
    
    env.execute()

  }

  class MyCoMap extends CoMapFunction[(Int, Long),
    (Int, String), String] {
    override def map1(in1: (Int, Long)): String = {
      "key为 " + in1._1 + " 的数据来自第一条流"
    }

    override def map2(in2: (Int, String)): String = {
      "key为 " + in2._1 + " 的数据来自第二条流"
    }

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.functions.co.CoFlatMapFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

object CoFlatMapExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment

    val stream1 = env
      .fromElements(
        (1, "first stream"),
        (2, "first stream")
      )
    
    val stream2 = env
      .fromElements(
        (1, "second stream"),
        (2, "second stream")
      )
    
    stream1
      .keyBy(_._1)
      .connect(stream2.keyBy(_._1))
      .flatMap(new MyFlatMap)
      .print()
    
    env.execute()

  }

  class MyFlatMap extends CoFlatMapFunction[(Int, String), (Int, String), String] {
    override def flatMap1(in1: (Int, String), collector: Collector[String]): Unit = {
      collector.collect(in1._2)
    }
    override def flatMap2(in2: (Int, String), collector: Collector[String]): Unit = {
      collector.collect(in2._2)
      collector.collect(in2._2)
    }
  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._

object SplitExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    val inputStream: DataStream[(Int, String)] = env
      .fromElements(
        (1001, "1001"),
        (999, "999")
      )

    val splitted: SplitStream[(Int, String)] = inputStream
      .split(t => if (t._1 > 1000) Seq("large") else Seq("small"))
    
    val large: DataStream[(Int, String)] = splitted.select("large")
    val small: DataStream[(Int, String)] = splitted.select("small")
    val all: DataStream[(Int, String)] = splitted.select("small", "large")
    
    large.print()
    
    env.execute()

  }
}
```



### Day02下午

```scala
package com.atguigu.datastreamapi

import org.apache.flink.api.common.functions.RichMapFunction
import org.apache.flink.configuration.Configuration
import org.apache.flink.streaming.api.scala._

object RichExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val stream = env
      .fromElements(1,2,3)
    
    stream
      .map(
        new RichMapFunction[Int, Int] {
          override def open(parameters: Configuration): Unit = {
            println("开始map的生命周期")
          }
    
          override def map(value: Int): Int = value + 1
    
          override def close(): Unit = {
            println("结束map的生命周期")
          }
        }
      )
      .print()
    
    env.execute()

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time

object WindowMinTemp {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    

    val readings = env
      .addSource(new SensorSource)
    
    readings
      .map(r => (r.id, r.temperature))
      .keyBy(_._1)

//      .timeWindow(Time.seconds(5))
      .timeWindow(Time.seconds(10), Time.seconds(5))
      .reduce((x1, x2) => (x1._1, x1._2.min(x2._2)))
      .print()

    env.execute()

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.api.common.functions.AggregateFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.time.Time

object AggregateExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val readings = env
      .addSource(new SensorSource)
      .map(r => (r.id, r.temperature))
      .keyBy(_._1)
      .timeWindow(Time.seconds(5))
      .aggregate(new AvgTemp)
    
    readings.print()
    env.execute()

  }

  class AvgTemp extends AggregateFunction[(String, Double),
    (String, Double, Long), (String, Double)] {
    override def createAccumulator(): (String, Double, Long) = {
      ("", 0.0, 0L)
    }
    override def add(value: (String, Double), accumulator: (String, Double, Long)): (String, Double, Long) = {
      (value._1, accumulator._2 + value._2, accumulator._3 + 1)
    }
    override def getResult(accumulator: (String, Double, Long)): (String, Double) = {
      (accumulator._1, accumulator._2 / accumulator._3)
    }
    override def merge(a: (String, Double, Long), b: (String, Double, Long)): (String, Double, Long) = {
      (a._1, a._2 + b._2, a._3 + b._3)
    }
  }
}
```

```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector

object ProcessWindowFunctionAvgTemp {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val readings = env
      .addSource(new SensorSource)
      .map(r => (r.id, r.temperature))
      .keyBy(_._1)
      .timeWindow(Time.seconds(5))
      .process(new MyAvgTemp)
    
    readings.print()
    env.execute()

  }

  class MyAvgTemp extends ProcessWindowFunction[
    (String, Double), (String, Double, Long), String, TimeWindow] {
    override def process(key: String,
                         context: Context,
                         elements: Iterable[(String, Double)],
                         out: Collector[(String, Double, Long)]): Unit = {
      var sum = 0.0
      val size = elements.size
      for (i <- elements) {
        sum += i._2
      }
      out.collect((key, sum / size, context.window.getEnd))
    }
  }
}
```

### Day03上午

```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector

object HighLowTemp {

  case class MinMaxTemp(id: String,
                        min: Double,
                        max: Double,
                        endTs: Long)

  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val readings = env.addSource(new SensorSource)
    
    readings
      .keyBy(r => r.id)
      .timeWindow(Time.seconds(5))
      .process(new HighLow)
      .print()
    
    env.execute()

  }

  class HighLow extends ProcessWindowFunction[SensorReading,
    MinMaxTemp, String, TimeWindow] {
    override def process(key: String,
                         context: Context,
                         elements: Iterable[SensorReading],
                         out: Collector[MinMaxTemp]): Unit = {
      val temps = elements.map(r => r.temperature)
      val endTs = context.window.getEnd
      out.collect(MinMaxTemp(key, temps.min, temps.max, endTs))
    }
  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.api.common.functions.AggregateFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector

object HighLowTempOptimize {

  case class MinMaxTemp(id: String,
                        min: Double,
                        max: Double,
                        endTs: Long)

  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val readings = env.addSource(new SensorSource)
    readings
      .keyBy(_.id)
      .timeWindow(Time.seconds(5))
      .aggregate(new AggTemp, new WindowTemp)
      .print()
    
    env.execute()

  }

  class WindowTemp extends ProcessWindowFunction[(String, Double, Double),
    MinMaxTemp, String, TimeWindow] {
    override def process(key: String,
                         context: Context,
                         elements: Iterable[(String, Double, Double)],
                         out: Collector[MinMaxTemp]): Unit = {
      val agg = elements.head
      out.collect(MinMaxTemp(key, agg._2, agg._3, context.window.getEnd))
    }
  }

  class AggTemp extends AggregateFunction[SensorReading,
    (String, Double, Double), (String, Double, Double)] {
    override def createAccumulator(): (String, Double, Double) = {
      ("", Double.MaxValue, Double.MinValue)
    }
    override def add(value: SensorReading, accumulator: (String, Double, Double)): (String, Double, Double) = {
      (value.id, accumulator._2.min(value.temperature), accumulator._3.max(value.temperature))
    }
    override def getResult(accumulator: (String, Double, Double)): (String, Double, Double) = {
      accumulator
    }
    override def merge(a: (String, Double, Double), b: (String, Double, Double)): (String, Double, Double) = {
      (a._1, a._2.min(b._2), a._3.max(b._3))
    }
  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector

object HighLowTempOptByReduce {
  case class MinMaxTemp(id: String,
                        min: Double,
                        max: Double,
                        endTs: Long)
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val readings = env.addSource(new SensorSource)
    readings
      .map(r => (r.id, r.temperature, r.temperature))
      .keyBy(_._1)
      .timeWindow(Time.seconds(5))
      .reduce(
        (r1, r2) => {
          (r1._1, r1._2.min(r2._2), r1._3.max(r2._3))
        },
        new WindowTemp
      )
      .print()
    
    env.execute()

  }

  class WindowTemp extends ProcessWindowFunction[(String, Double, Double),
    MinMaxTemp, String, TimeWindow] {
    override def process(key: String,
                         context: Context,
                         elements: Iterable[(String, Double, Double)],
                         out: Collector[HighLowTempOptByReduce.MinMaxTemp]): Unit = {
      val agg = elements.head
      out.collect(MinMaxTemp(key, agg._2, agg._3, context.window.getEnd))
    }
  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector

object HighLowTempOptByReduce {
  case class MinMaxTemp(id: String,
                        min: Double,
                        max: Double,
                        endTs: Long)
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val readings = env.addSource(new SensorSource)
    readings
      .map(r => (r.id, r.temperature, r.temperature))
      .keyBy(_._1)
      .timeWindow(Time.seconds(5))
      .reduce(
        (r1: (String, Double, Double), r2: (String, Double, Double)) => {
          (r1._1, r1._2.min(r2._2), r1._3.max(r2._3))
        },
        new WindowTemp
      )
      .print()
    
    env.execute()

  }

  class WindowTemp extends ProcessWindowFunction[(String, Double, Double),
    MinMaxTemp, String, TimeWindow] {
    // process函数什么时候调用？
    // 当窗口闭合的时候调用
    override def process(key: String,
                         context: Context,
                         elements: Iterable[(String, Double, Double)],
                         out: Collector[HighLowTempOptByReduce.MinMaxTemp]): Unit = {
      val agg = elements.head
      out.collect(MinMaxTemp(key, agg._2, agg._3, context.window.getEnd))
    }
  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector

object EventTimeExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.setParallelism(1)

    val stream = env
      .socketTextStream("localhost", 9999, '\n')
      .map(line => {
        val arr = line.split(" ")
        (arr(0), arr(1).toLong * 1000)
      })
      // 抽取时间戳（单位必须是ms），插入水位线
      // 水位线 = 观察到的最大事件时间 - 最大延迟时间
      .assignTimestampsAndWatermarks(
        // 设置最大延迟时间是5s
        new BoundedOutOfOrdernessTimestampExtractor[(String, Long)](Time.seconds(5)) {
          override def extractTimestamp(t: (String, Long)): Long = t._2
        }
      )
      .keyBy(_._1)
      .timeWindow(Time.seconds(10))
      .process(new EvenTimeProcess)
    
    stream.print()
    env.execute()

  }

  class EvenTimeProcess extends ProcessWindowFunction[(String, Long),
    String, String, TimeWindow] {
    override def process(key: String,
                         context: Context,
                         elements: Iterable[(String, Long)],
                         out: Collector[String]): Unit = {
      out.collect("窗口结束时间为：" + context.window.getEnd + " 窗口共有 " + elements.size + " 条数据")
    }
  }
}
```



```scala
env.getConfig.setAutoWatermarkInterval(60 * 1000)
```

### Day03下午

```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.watermark.Watermark
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector

object PeriodicWatermarkExample {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.setParallelism(1)
    

    val stream = env
      .addSource(new SensorSource)
      .assignTimestampsAndWatermarks(new MyAssigner)
        .keyBy(_.id)
        .timeWindow(Time.seconds(5))
        .process(new MyProcess)
    
    stream.print()
    env.execute()

  }

  class MyProcess extends ProcessWindowFunction[SensorReading, String, String, TimeWindow] {
    override def process(key: String,
                         context: Context,
                         elements: Iterable[SensorReading],
                         out: Collector[String]): Unit = {
      out.collect("hello world")
    }
  }

  class MyAssigner extends AssignerWithPeriodicWatermarks[SensorReading] {
    val bound: Long = 60 * 1000 // 最大延迟时间
    var maxTs: Long = Long.MinValue + bound // 用来保存观察到的最大事件时间
    

    // 每来一条数据，调用一次
    override def extractTimestamp(element: SensorReading, previousElementTimestamp: Long): Long = {
      maxTs = maxTs.max(element.timestamp) // 更新观察到的最大事件时间
      element.timestamp // 提取事件时间戳
    }
    
    // 系统插入水位线的时候调用
    override def getCurrentWatermark: Watermark = {
      // 水位线 = 观察到的最大事件时间 - 最大延迟时间
      new Watermark(maxTs - bound)
    }

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.co.CoProcessFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

object DistributedWatermark {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

    val stream1 = env
      .socketTextStream("localhost", 9999, '\n')
      .map(line => {
        val arr = line.split(" ")
        (arr(0), arr(1).toLong * 1000)
      })
      .assignAscendingTimestamps(_._2)
      .keyBy(_._1)


    val stream2 = env
      .socketTextStream("localhost", 9998, '\n')
      .map(line => {
        val arr = line.split(" ")
        (arr(0), arr(1).toLong * 1000)
      })
      .assignAscendingTimestamps(_._2)
      .keyBy(_._1)
    
    stream1.connect(stream2).process(new MyCoProcess).print()
    env.execute()

  }
  class MyCoProcess extends CoProcessFunction[(String, Long), (String, Long), String] {
    override def processElement1(value: (String, Long),
                                 ctx: CoProcessFunction[(String, Long), (String, Long), String]#Context,
                                 out: Collector[String]): Unit = {
      println("from stream1", ctx.timerService().currentWatermark())
      out.collect("from stream1")
    }

    override def processElement2(value: (String, Long),
                                 ctx: CoProcessFunction[(String, Long), (String, Long), String]#Context, out: Collector[String]): Unit = {
      println("from stream2", ctx.timerService().currentWatermark())
      out.collect("from stream2")
    
    }

  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.functions.KeyedProcessFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

object TestOnTimerProcessingTime {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val stream = env
      .addSource(new SensorSource)
      .keyBy(_.id)
      .process(new Key)
    
    stream.print()
    env.execute()

  }
  class Key extends KeyedProcessFunction[String, SensorReading, SensorReading] {
    override def processElement(value: SensorReading,
                                ctx: KeyedProcessFunction[String, SensorReading, SensorReading]#Context,
                                out: Collector[SensorReading]): Unit = {
      out.collect(value)
      ctx.timerService().registerProcessingTimeTimer(value.timestamp + 10 * 1000L)
    }
    override def onTimer(timestamp: Long, ctx: KeyedProcessFunction[String, SensorReading, SensorReading]#OnTimerContext, out: Collector[SensorReading]): Unit = {
      println("定时事件执行了！！！")
    }
  }
}
```



```scala
package com.atguigu.datastreamapi

import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.KeyedProcessFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

object TestOnTimer {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.setParallelism(1)

    val stream = env
      .socketTextStream("localhost", 9999, '\n')
      .map(line => {
        val arr = line.split(" ")
        (arr(0), arr(1).toLong * 1000)
      })
      .assignAscendingTimestamps(_._2)
      .keyBy(_._1)
      .process(new Key)
    
    stream.print()
    env.execute()

  }
  class Key extends KeyedProcessFunction[String, (String, Long), String] {
    override def processElement(value: (String, Long),
                                ctx: KeyedProcessFunction[String, (String, Long), String]#Context,
                                out: Collector[String]): Unit = {
      ctx.timerService().registerEventTimeTimer(value._2 + 10 * 1000L - 1)
    }
    override def onTimer(timestamp: Long,
                         ctx: KeyedProcessFunction[String, (String, Long), String]#OnTimerContext,
                         out: Collector[String]): Unit = {
      println("定时事件发生的时间：" + timestamp)
      out.collect("定时事件发生了！！！")
    }
  }
}
```

```scala
package com.atguigu.datastreamapi

import org.apache.flink.api.common.state.ValueStateDescriptor
import org.apache.flink.api.scala.typeutils.Types
import org.apache.flink.streaming.api.functions.KeyedProcessFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

object TempIncreaseAlert {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)

    val readings = env
      .addSource(new SensorSource)
      .keyBy(_.id)
      .process(new TempIncreaseAlertFunction)
    
    readings.print()
    env.execute()

  }

  class TempIncreaseAlertFunction extends KeyedProcessFunction[String, SensorReading, String] {
    lazy val lastTemp = getRuntimeContext.getState(
      new ValueStateDescriptor[Double]("last-temp", Types.of[Double])
    )
    lazy val currentTimer = getRuntimeContext.getState(
      new ValueStateDescriptor[Long]("timer", Types.of[Long])
    )
    override def processElement(r: SensorReading,
                                ctx: KeyedProcessFunction[String, SensorReading, String]#Context,
                                out: Collector[String]): Unit = {
      val prevTemp = lastTemp.value()
      lastTemp.update(r.temperature)

      val curTimerTimestamp = currentTimer.value()
    
      if (prevTemp == 0.0 || r.temperature < prevTemp) {
        ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp)
        currentTimer.clear()
      } else if (r.temperature > prevTemp && curTimerTimestamp == 0L) {
        val timerTs = ctx.timerService().currentProcessingTime() + 1000L
        ctx.timerService().registerProcessingTimeTimer(timerTs)
        currentTimer.update(timerTs)
      }
    }
    override def onTimer(ts: Long,
                         ctx: KeyedProcessFunction[String, SensorReading, String]#OnTimerContext,
                         out: Collector[String]): Unit = {
      out.collect("温度连续1s上升了！" + ctx.getCurrentKey)
    }

  }
}
```

