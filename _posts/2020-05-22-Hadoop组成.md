---
layout: post
title: "[Hadoop] 组成架构"
subtitle: 'Zookeeper3.4.10'
date: 2020-05-23 11:11:00
author: "Backspace"
header-img: "img/post-bg-apple-event-2015.jpg"
tags:
  - Hadoop
  - 组成架构
---

## Hadoop组合拳

### 1. 三大发行版本

Apache	Cloudera	Hortonworks

### 2. Hadoop 1.x和2.x的区别

`在Hadoop1.x的时代，Hadoop中的MapReduce同时处理业务逻辑运算和资源调度，耦合性较大。2.x时代增加了Yarn，Yarn只负责资源调度，MapReduce只负责计算`

### 3. Hadoop编译源码

### 4. 常用端口号

|             功能             | hadoop2.x | hadoop3.x |
| :--------------------------: | :-------: | :-------: |
|      访问历史服务器端口      |   19888   |   19888   |
|      访问MapReduce端口       |   8088    |   8088    |
| 内部通信[客户端访问集群端口] |  `9000`   |  `8020`   |
|         访问hdfs端口         |  `50070`  |  `9870`   |

### 5. hadoop配置文件

hadoop2.x



hadoop3.x

### 6. 简单的集群搭建

1. JDK安装
2. 配置SSH免密登录
3. 配置hadoop核心文件
4. 格式化namenode

### 7. 项目经验之基准测试	

搭建完Hadoop集群后需要对HDFS读写性能和MR计算能力测试。测试jar包在hadoop的share文件夹下。

测试方法：

### 8. Hadoop宕机	



### 9. Hadoop解决数据倾斜方法



### 10. 集群资源分配参数（项目中遇到的问题）



## HDFS

> HDFS是一个`分布式的文件管理系统`，适合`一次写入多次读取`的场景，`不支持文件修改`。hdfs具有`高容错性`，会给数据创建副本，有副本丢失以后还能自动修复，`适合处理大数据`，`副本机制`让hdfs可以搭载在`廉价`的机器上，通过副本机制来提高`可靠性`。然而hdfs`不适合低延时的数据访问`，也`没有办法对大量小文件进行高效的存储`，而且`仅支持数据的追加操作`，`不支持并发写入`和`随机修改`。

### 1. 组成架构

hdfs由NamNode，DataNode，Client，SecondaryNameNode组成，个部分功能：

### 2. 文件块大小

2.x版本默认`128M`，老版本是`64M`，可以自己修改`hdfs-defaul.xml`文件内的`dfs.blocksize`来设置块大小。如果文件块太小，会增加寻址时间，程序寻址时间大于存储时间，会一直处于找文件块开始位置的状态。如果文件块太大，从磁盘传输数据的时间远远大于定位这个块开始位置使用的时间，导致程序在处理数据时非常慢。`HDFS块的大小设置主要取决于磁盘传输速率`。经试验`寻址时间 = 传输时间 x 1%`时是最佳状态。

### 3.shell操作[ 开发 ]

**基本语法**

`bin/hadoop fs  具体命令`  或者  `bin/hdfs dfs 具体命令` dfs是fs的实现类

**常用命令实操**

启动Hadoop集群（方便后续的测试）

- `[atguigu@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh`
- `[atguigu@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh`

`-help`：输出这个命令参数

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -help rm`

`-ls`: 显示目录信息

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /`

`-mkdir`：在HDFS上创建目录

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /sanguo/shuguo`

`-moveFromLocal`：从本地剪切粘贴到HDFS

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo`

`-appendToFile：`追加一个文件到已经存在的文件末尾

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt`

`-cat`：显示文件内容

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /sanguo/shuguo/kongming.txt`

`-chgrp` 、`-chmod`、`-chown`：Linux文件系统中的用法一样，修改文件所属权限

- `[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt`
- `[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -chown atguigu:atguigu  /sanguo/shuguo/kongming.txt`

`-copyFromLocal`：从本地文件系统中拷贝文件到HDFS路径去

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /`

`-copyToLocal`：从HDFS拷贝到本地

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./`

`-cp` ：从HDFS的一个路径拷贝到HDFS的另一个路径

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt`

`-mv`：在HDFS目录中移动文件

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /zhuge.txt /sanguo/shuguo/`

`-get`：等同于copyToLocal，就是从HDFS下载文件到本地

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -get /sanguo/shuguo/kongming.txt ./`

`-getmerge`：合并下载多个文件，比如HDFS的目录 /user/atguigu/test下有多个文件:log.1, log.2,log.3,...

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /user/atguigu/test/* ./zaiyiqi.txt`

`-put`：等同于copyFromLocal

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -put ./zaiyiqi.txt /user/atguigu/test/`

`-tail`：显示一个文件的末尾

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -tail /sanguo/shuguo/kongming.txt`

`-rm`：删除文件或文件夹

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /user/atguigu/test/jinlian2.txt`

`-rmdir`：删除空目录

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test`

`-du`统计文件夹的大小信息

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /user/atguigu/test`

`-setrep`：设置HDFS中文件的副本数量

`[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt`

这里设置的副本数只是`记录在NameNode的元数据中`，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。

### 4.客户端操作[ 开发 ]

客户端环境准备

4．创建一个Maven工程HdfsClientDemo
5．导入相应的依赖坐标+日志添加

注意：如果Eclipse/Idea打印不出日志，在控制台上只显示
1.log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).  
2.log4j:WARN Please initialize the log4j system properly.  
3.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
需要在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入
log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
6．创建包名：com.atguigu.hdfs
7．创建HdfsClient类
public class HdfsClient{	
@Test
public void testMkdirs() throws IOException, InterruptedException, URISyntaxException{
		
		// 1 获取文件系统
		Configuration configuration = new Configuration();
		// 配置在集群上运行
		// configuration.set("fs.defaultFS", "hdfs://hadoop102:9000");
		// FileSystem fs = FileSystem.get(configuration);
	
		FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
		// 2 创建目录
		fs.mkdirs(new Path("/1108/daxian/banzhang"));
		
		// 3 关闭资源
		fs.close();
	}
}
8．执行程序
运行时需要配置用户名称，如图3-7所示

图3-7  配置用户名称
客户端去操作HDFS时，是有一个用户身份的。默认情况下，HDFS客户端API会从JVM中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=atguigu，atguigu为用户名称。

API操作

3.2.1 HDFS文件上传（测试参数优先级）
1．编写源代码
@Test
public void testCopyFromLocalFile() throws IOException, InterruptedException, URISyntaxException {

		// 1 获取文件系统
		Configuration configuration = new Configuration();
		configuration.set("dfs.replication", "2");
		FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
	
		// 2 上传文件
		fs.copyFromLocalFile(new Path("e:/banzhang.txt"), new Path("/banzhang.txt"));
	
		// 3 关闭资源
		fs.close();
	
		System.out.println("over");
}
2．将hdfs-site.xml拷贝到项目的根目录下
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<property>
		<name>dfs.replication</name>
        <value>1</value>
	</property>
</configuration>
3．参数优先级
参数优先级排序：（1）客户端代码中设置的值 >（2）ClassPath下的用户自定义配置文件 >（3）然后是服务器的默认配置
3.2.2 HDFS文件下载
@Test
public void testCopyToLocalFile() throws IOException, InterruptedException, URISyntaxException{

		// 1 获取文件系统
		Configuration configuration = new Configuration();
		FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
		// 2 执行下载操作
		// boolean delSrc 指是否将原文件删除
		// Path src 指要下载的文件路径
		// Path dst 指将文件下载到的路径
		// boolean useRawLocalFileSystem 是否开启文件校验
		fs.copyToLocalFile(false, new Path("/banzhang.txt"), new Path("e:/banhua.txt"), true);
		
		// 3 关闭资源
		fs.close();
}
3.2.3 HDFS文件夹删除
@Test
public void testDelete() throws IOException, InterruptedException, URISyntaxException{

	// 1 获取文件系统
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
	// 2 执行删除
	fs.delete(new Path("/0508/"), true);
		
	// 3 关闭资源
	fs.close();
}
3.2.4 HDFS文件名更改
@Test
public void testRename() throws IOException, InterruptedException, URISyntaxException{

	// 1 获取文件系统
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu"); 
		
	// 2 修改文件名称
	fs.rename(new Path("/banzhang.txt"), new Path("/banhua.txt"));
		
	// 3 关闭资源
	fs.close();
}
3.2.5 HDFS文件详情查看
查看文件名称、权限、长度、块信息
@Test
public void testListFiles() throws IOException, InterruptedException, URISyntaxException{

	// 1获取文件系统
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu"); 
		
	// 2 获取文件详情
	RemoteIterator<LocatedFileStatus> listFiles = fs.listFiles(new Path("/"), true);
		
	while(listFiles.hasNext()){
		LocatedFileStatus status = listFiles.next();
			
		// 输出详情
		// 文件名称
		System.out.println(status.getPath().getName());
		// 长度
		System.out.println(status.getLen());
		// 权限
		System.out.println(status.getPermission());
		// 分组
		System.out.println(status.getGroup());
			
		// 获取存储的块信息
		BlockLocation[] blockLocations = status.getBlockLocations();
			
		for (BlockLocation blockLocation : blockLocations) {
				
			// 获取块存储的主机节点
			String[] hosts = blockLocation.getHosts();
				
			for (String host : hosts) {
				System.out.println(host);
			}
		}
			
		System.out.println("-----------班长的分割线----------");
	}

// 3 关闭资源
fs.close();
}
3.2.6 HDFS文件和文件夹判断
@Test
public void testListStatus() throws IOException, InterruptedException, URISyntaxException{
		
	// 1 获取文件配置信息
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
	// 2 判断是文件还是文件夹
	FileStatus[] listStatus = fs.listStatus(new Path("/"));
		
	for (FileStatus fileStatus : listStatus) {
		
		// 如果是文件
		if (fileStatus.isFile()) {
				System.out.println("f:"+fileStatus.getPath().getName());
			}else {
				System.out.println("d:"+fileStatus.getPath().getName());
			}
		}
		
	// 3 关闭资源
	fs.close();
}

I/O流操作

1．需求：把本地e盘上的banhua.txt文件上传到HDFS根目录
2．编写代码
@Test
public void putFileToHDFS() throws IOException, InterruptedException, URISyntaxException {

	// 1 获取文件系统
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
	
	// 2 创建输入流
	FileInputStream fis = new FileInputStream(new File("e:/banhua.txt"));
	
	// 3 获取输出流
	FSDataOutputStream fos = fs.create(new Path("/banhua.txt"));
	
	// 4 流对拷
	IOUtils.copyBytes(fis, fos, configuration);
	
	// 5 关闭资源
	IOUtils.closeStream(fos);
	IOUtils.closeStream(fis);
	fs.close();
}
3.3.2 HDFS文件下载
1．需求：从HDFS上下载banhua.txt文件到本地e盘上
2．编写代码
// 文件下载
@Test
public void getFileFromHDFS() throws IOException, InterruptedException, URISyntaxException{

	// 1 获取文件系统
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
	// 2 获取输入流
	FSDataInputStream fis = fs.open(new Path("/banhua.txt"));
		
	// 3 获取输出流
	FileOutputStream fos = new FileOutputStream(new File("e:/banhua.txt"));
		
	// 4 流的对拷
	IOUtils.copyBytes(fis, fos, configuration);
		
	// 5 关闭资源
	IOUtils.closeStream(fos);
	IOUtils.closeStream(fis);
	fs.close();
}
3.3.3 定位文件读取
1．需求：分块读取HDFS上的大文件，比如根目录下的/hadoop-2.7.2.tar.gz
2．编写代码
（1）下载第一块
@Test
public void readFileSeek1() throws IOException, InterruptedException, URISyntaxException{

	// 1 获取文件系统
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
	// 2 获取输入流
	FSDataInputStream fis = fs.open(new Path("/hadoop-2.7.2.tar.gz"));
		
	// 3 创建输出流
	FileOutputStream fos = new FileOutputStream(new File("e:/hadoop-2.7.2.tar.gz.part1"));
		
	// 4 流的拷贝
	byte[] buf = new byte[1024];
		
	for(int i =0 ; i < 1024 * 128; i++){
		fis.read(buf);
		fos.write(buf);
	}
		
	// 5关闭资源
	IOUtils.closeStream(fis);
	IOUtils.closeStream(fos);
fs.close();
}
（2）下载第二块
@Test
public void readFileSeek2() throws IOException, InterruptedException, URISyntaxException{

	// 1 获取文件系统
	Configuration configuration = new Configuration();
	FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu");
		
	// 2 打开输入流
	FSDataInputStream fis = fs.open(new Path("/hadoop-2.7.2.tar.gz"));
		
	// 3 定位输入数据位置
	fis.seek(1024*1024*128);
		
	// 4 创建输出流
	FileOutputStream fos = new FileOutputStream(new File("e:/hadoop-2.7.2.tar.gz.part2"));
		
	// 5 流的对拷
	IOUtils.copyBytes(fis, fos, configuration);
		
	// 6 关闭资源
	IOUtils.closeStream(fis);
	IOUtils.closeStream(fos);
}
（3）合并文件
在Window命令窗口中进入到目录E:\，然后执行如下命令，对数据进行合并
type hadoop-2.7.2.tar.gz.part2 >> hadoop-2.7.2.tar.gz.part1
合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。

### 5. 读流程和写流程

**hdfs写数据流程**

![](i:\Temp\github\backspace2019.github.io\img\in-post\2020-05\Hadoop-write.png)

1. 客户端通过`Distributed FileSystem`模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。
2. NameNode返回是否可以上传。
3. 客户端请求第一个 Block上传到哪几个DataNode服务器上。
4. NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。
5. 客户端通过`FSDataOutputStream`模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。
6. dn1、dn2、dn3逐级应答客户端。
7. 客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。
8. 当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。

- **网络拓扑--节点间的距**

  两个节点到达最近的共同祖先的距离总和

- **机架感知**

  副本存储节点选择：第一个副本 本机，第二个副本 本机架随机，第三个副本 其它机架随机。

**hdfs读数据流程**

![](i:\Temp\github\backspace2019.github.io\img\in-post\2020-05\Hadoop-read.png)

1. 客户端通过`Distributed FileSystem`向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。
2. 挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。

### 6. NN和2NN工作机制

问题起源于NameNode的元数据存放在哪里，假如存放在磁盘中，效率低，所以存放在内存中FsImage，但是存放在内存中，又要考虑更新和断电是否安全的问题，更新可以用Edit文件去更新，但是需要定时经常更新，很繁琐，所以SecondaryNamenode，专门用于FsImage和Edits的合并。一旦有新的变化，先存在Edit文件中，定时跟fsIamge合并文件，就能更新数据了。

![](i:\Temp\github\backspace2019.github.io\img\in-post\2020-05\NN2NN.png)

第一阶段，NameNode启动

1. 第一次启动NameNode格式化以后产生FsImage和Edit文件，如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
2. 客户端对原数据进行增删改的请求
3. NameNode记录操作日志，更新滚动日志
4. NameNode在内存中对元数据进行增删改

第二阶段，SecondaryNameNode工作

1. SecondaryNameNode询问NameNode是否需要CheckPoint，直接带回NameNode的结果。触发CheckPoint需要满足两个条件中的任意一个，`定时时间到`和`Edits中数据写满`了
2. SecondaryNameNode请求执行CheckPoint
3. NameNode 滚动正在写的Edits日志
4. 将滚动前的日志和镜像文件拷贝到SecondaryNameNode
5. SecondaryNameNode加载日志和镜像文件到内存，并合并
6. 生成新的镜像文件fsimage.chkpoint
7. 拷贝fsimage.chkpoint到NameNode
8. NameNode将fsimage.chkpoint重新命名成fsimage

### 7.NN和2NN工作的其它细节

- FsImage和Edits解析

  NameNode格式化之后会产生4个文件：

  ```shell
  /opt/module/hadoop-2.7.2/data/tmp/dfs/name/current
  fsimage_0000000000000000000 #永久性检查点
  fsimage_0000000000000000000.md5 #永久性检查点
  seen_txid #最后一个edits_的数字
  VERSION 
  ```

- CheckPoint时间设置

  通常情况下，SecondaryNameNode每隔一小时执行一次

  ```xml
  <!--[hdfs-default.xml]-->
  <property>
   <name>dfs.namenode.checkpoint.period</name>
   <value>3600</value>
  </property>
  ```

  一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。

  ```xml
  <property>
   <name>dfs.namenode.checkpoint.txns</name>
   <value>1000000</value>
  <description>操作动作次数</description>
  </property>
   
  <property>
   <name>dfs.namenode.checkpoint.check.period</name>
   <value>60</value>
  <description> 1分钟检查一次操作次数</description>
  </property >
  ```

- NameNode故障处理

  将SecondaryNameNode中数据拷贝到NameNode存储数据的目录

  ```shell
  #1. kill -9 NameNode进程
  #2. 删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）
  [atguigu@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*
  #3. 拷贝SecondaryNameNode中数据到原NameNode存储数据目录
  [atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary/* ./name/
  #4. 重新启动NameNode
  [atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode
  ```

  使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中。

  ```xml
  <!--1.  修改hdfs-site.xml中的-->
  <property>
   <name>dfs.namenode.checkpoint.period</name>
   <value>120</value>
  </property>
  
  <property>
   <name>dfs.namenode.name.dir</name>
   <value>/opt/module/hadoop-2.7.2/data/tmp/dfs/name</value>
  </property>
  ```

  ```shell
  #2. kill -9 NameNode进程
  
  #3.  删除NameNode存储的数据（/opt/module/hadoop-2.7.2/data/tmp/dfs/name）
  [atguigu@hadoop102 hadoop-2.7.2]$ rm -rf /opt/module/hadoop-2.7.2/data/tmp/dfs/name/*
  
  #4.  如果SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录，并删除in_use.lock文件
  [atguigu@hadoop102 dfs]$ scp -r atguigu@hadoop104:/opt/module/hadoop-2.7.2/data/tmp/dfs/namesecondary ./
  
  [atguigu@hadoop102 namesecondary]$ rm -rf in_use.lock
  
  [atguigu@hadoop102 dfs]$ pwd
  /opt/module/hadoop-2.7.2/data/tmp/dfs
  
  [atguigu@hadoop102 dfs]$ ls
  data name namesecondary
  
  #5.  导入检查点数据（等待一会ctrl+c结束掉）
  [atguigu@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -importCheckpoint
  
  #6.  启动NameNode
  [atguigu@hadoop102 hadoop-2.7.2]$ sbin/hadoop-daemon.sh start namenode
  ```

- 集群安全模式

  `NameNode启动`，启动之后到生成新的FSimage和空的编辑日志之间，NameNode一直处于安全模式，对客户端只读

  `DataNode启动`，系统中的数据块的位置并不是由NameNode维护的，而是以块列表的形式存储在DataNode中。正常操作时，NameNode会在内存中保留所有块位置信息映射。在安全模式下，各个DataNode会向NameNode发送最新的块列表信息，NameNode了解了足够多的块位置信息之后就能高效工作了

  退出安全模式判断：满足`最小副本条件`，NameNode就会在30 秒内自动退出安全模式。文件系统中99.9%的块满足最小副本级别。`默认dfs.replication.min=1`，启动一个刚刚格式化的hdfs集群时，因为没有任何块所以不会进安全模式

- NameNode多目录配置

  每个目录存放内容相同，提高可靠性。

### 8.DataNode工作机制

![](G:\Temp\github\backspace2019.github.io\img\in-post\2020-05\DataNode.png)

- 一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是`数据本身`，一个是`元数据=数据块长度+块数据校验和+时间戳`。
- DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。
- `心跳是每3秒一次`，心跳返回结果带有`NameNode给该DataNode的命令`如复制块数据到另一台机器，或删除某个数据块。如果`超过10分钟`没有收到某个DataNode的心跳，则认为该节点不可用。
- 集群运行中可以安全加入和退出一些机器。

### 9.DataNode工作的其它细节

- 数据完整性

  `checksum`校验和

- 掉线时限参数设置

  HDFS默认的超时时长是10分钟30秒=2 x `dfs.namenode.heartbeat.recheck-interval`+10 x `dfs.heartbeat.interval`

  `dfs.namenode.heartbeat.recheck-interval`默认5分钟

  `dfs.heartbeat.interval`默认3秒

- 服役新数据节点

  数据不平衡时采用`[atguigu@hadoop102 sbin]$ ./start-balancer.sh`

- 退役旧数据节点

  添加白名单，不在白名单的主机节点都会被退役

  ```shell
  #配置白名单的具体步骤如下：
  #在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts文件
  [atguigu@hadoop102 hadoop]$ pwd
  #/opt/module/hadoop-2.7.2/etc/hadoop
  [atguigu@hadoop102 hadoop]$ touch dfs.hosts
  [atguigu@hadoop102 hadoop]$ vi dfs.hosts
  #添加如下主机名称（不添加hadoop105）
  hadoop102
  hadoop103
  hadoop104
  #（2）在NameNode的hdfs-site.xml配置文件中增加dfs.hosts属性
  <property>
  <name>dfs.hosts</name>
  <value>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts</value>
  </property>
  #（3）配置文件分发
  [atguigu@hadoop102 hadoop]$ xsync hdfs-site.xml
  #（4）刷新NameNode
  [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes
  #（5）更新ResourceManager节点
  [atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes
  ```

  黑名单退役，黑名单的主机节点都会被强制退役

  ```shell
  #1.在NameNode的/opt/module/hadoop-2.7.2/etc/hadoop目录下创建dfs.hosts.exclude文件
  [atguigu@hadoop102 hadoop]$ pwd
  #/opt/module/hadoop-2.7.2/etc/hadoop
  [atguigu@hadoop102 hadoop]$ touch dfs.hosts.exclude
  [atguigu@hadoop102 hadoop]$ vi dfs.hosts.exclude
  #添加如下主机名称（要退役的节点）
  # hadoop105
  #2．在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性
  <property>
  <name>dfs.hosts.exclude</name>
     <value>/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude</value>
  </property>
  #3．刷新NameNode、刷新ResourceManager
  [atguigu@hadoop102 hadoop-2.7.2]$ hdfs dfsadmin -refreshNodes
  [atguigu@hadoop102 hadoop-2.7.2]$ yarn rmadmin -refreshNodes
  ```

- DataNode多目录配置

  DataNode也可以配置成多个目录，每个目录存储的数据`不一样`。即：数据不是副本

  ```xml
  <!--hdfs-site.xml-->
  <property>
  <name>dfs.datanode.data.dir</name>
  <value>file:///${hadoop.tmp.dir}/dfs/data1,file:///${hadoop.tmp.dir}/dfs/data2</value>
  </property>
  ```

### 10. 集群间数据拷贝

- scp实现两个远程主机之间的文件复制

  `scp -r hello.txt root@hadoop103:/user/atguigu/hello.txt`     // 推 push

  `scp -r root@hadoop103:/user/atguigu/hello.txt hello.txt`     // 拉 pull

  `scp -r root@hadoop103:/user/atguigu/hello.txt root@hadoop104:/user/atguigu`  //是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。

- 采用distcp命令实现hadoop集群之间的递归数据复制

  `[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop distcp`

### 11. 小文件处理

- 采用HAR归档方式，将小文件归档，内部还是一个个文件，对NameNode来说是一个整体，减少NameNode内存
- 采用CombineTextInputFormat
- 有小文件场景开启JVM重用，没有的不用开启，因为会一直占用使用的task卡槽，直到任务完成才释放。JVM重用可以是的JVM在同一个job中重新使用N次，N的值可以在mapred-site.xml中设置，一般在10-20之间

```shell
#HAR归档
#（1）需要启动YARN进程
[atguigu@hadoop102 hadoop-2.7.2]$ start-yarn.sh
#（2）归档文件
#把/user/atguigu/input目录里面的所有文件归档成一个叫input.har的归档文件，并把归档后文件存储到/user/atguigu/output路径下。
[atguigu@hadoop102 hadoop-2.7.2]$ bin/hadoop archive -archiveName input.har –p /user/atguigu/input  /user/atguigu/output
#（3）查看归档
[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr /user/atguigu/output/input.har
[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -lsr har:///user/atguigu/output/input.har
#（4）解归档文件
[atguigu@hadoop102 hadoop-2.7.2]$ hadoop fs -cp har:/// user/atguigu/output/input.har/*  /user/atguigu
```

## Yarn

### 1. 工作机制



### 2. 资源调度器



## MapReduce

1.InputFormat数据输入

2.MapReduce工作流

### 3. shuffle及优化

4.MapTask

5.ReduceTask

6.OutputFormat

7.Join多种应用

8.计数器应用

9.数据清洗ETL

10.开发总结

### 2.业务逻辑怎么编写

11.数据压缩

12.Yarn资源调度器

13.企业优化

14.扩展案例

## Hbase

### 1. HBase存储结构



### 2. RowKey设计原则



### 3. RowKey如何设计



## Phoenix

### 1. Phoenix二级索引（讲原理）

